{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db208f48-c315-41f2-b5da-e1464a1ee961",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8275e7dd-5201-4759-8b16-367ae6fff4ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/Siobhan/Desktop/acvdl_final'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import json\n",
    "import pydicom\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Setup:\n",
    "# root_dir should be folder containing 'fracactlas' tar file or folder and\n",
    "# UNIFEST zip file or folder\n",
    "\n",
    "root_dir = os.path.join(os.path.expanduser('~'), 'Desktop', 'acvdl_final')\n",
    "frac_dir = os.path.join(root_dir, 'fracatlas-DatasetNinja')\n",
    "uni_dir = os.path.join(root_dir, 'archive')\n",
    "total_dir = os.path.join(root_dir, 'total_images')\n",
    "os.chdir(root_dir)\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c56c7e6a-98ec-4873-827a-2d08045208f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "def extract_zip(zip_path, extract_to='.'):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "        print(f'Extracted {zip_path} to {extract_to}')\n",
    "\n",
    "import tarfile\n",
    "\n",
    "def extract_tar(tar_path, extract_to='.'):\n",
    "    with tarfile.open(tar_path, 'r:*') as tar_ref:\n",
    "        tar_ref.extractall(extract_to)\n",
    "        print(f'Extracted {tar_path} to {extract_to}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd092a2-6425-4a63-bb6a-272b9af36e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting existing fracatlas dir\n",
      "Extracted fracatlas-DatasetNinja.tar to fracatlas-DatasetNinja\n",
      "Deleting existing UNIFEST dir\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(frac_dir):\n",
    "    print('Deleting existing fracatlas dir')\n",
    "    shutil.rmtree(frac_dir)\n",
    "    extract_tar('fracatlas-DatasetNinja.tar', extract_to='fracatlas-DatasetNinja')\n",
    "if os.path.exists(uni_dir):\n",
    "    print('Deleting existing UNIFEST dir')\n",
    "    shutil.rmtree(uni_dir) \n",
    "    extract_zip('archive.zip', extract_to='archive')\n",
    "if os.path.exists(total_dir):\n",
    "    shutil.rmtree(total_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea1de53-f1c5-45d6-9bb3-40e6f42980a1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Fracatlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b90ed6-73a6-45c7-a7f6-9f15b1c7b78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(frac_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a1494e-5130-40b2-9be2-f1b428437e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree -L 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7755737-323e-4c5e-b24d-6261d22d55ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "!find \"not fractured\" -type f -name \"*.json\" | wc -l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47e705e-db62-407a-a82b-00c2a969cb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!find \"test\" -type f -name \"*.json\" | wc -l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2f7b4f-e3e7-45c4-be9f-f86ea9cd8510",
   "metadata": {},
   "outputs": [],
   "source": [
    "!find \"train\" -type f -name \"*.json\" | wc -l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013ef9d1-e9ed-4550-932d-a31d886f5dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!find \"val\" -type f -name \"*.json\" | wc -l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c37f09-0024-47eb-9ff3-e8f157475ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_prop = 82/(574+82)\n",
    "\n",
    "print(val_prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e89152-acb2-44d3-a35a-10766d5bb1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exmaple json\n",
    "\n",
    "file = os.path.join(frac_dir, 'test/ann/IMG0003297.jpg.json')\n",
    "\n",
    "with open(file) as json_data:\n",
    "    d = json.load(json_data)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116738e9-699b-4930-bf7d-ca5cd1a61737",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import json\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "os.chdir(frac_dir)\n",
    "os.getcwd()\n",
    "\n",
    "fracdata = pd.DataFrame(columns=['image_file_name', 'Target', 'height', 'width', 'File_path', 'ttv_type', 'dataset'])\n",
    "\n",
    "for root, dirs, filenames in os.walk(\".\"):\n",
    "    if \"not fractured\" in dirs:\n",
    "        dirs.remove(\"not fractured\")\n",
    "\n",
    "    if \"img\" in dirs:\n",
    "        dirs.remove(\"img\")\n",
    "\n",
    "    for file in filenames:      \n",
    "        if file.startswith(\".\"):\n",
    "            continue\n",
    "\n",
    "        filepath = os.path.join(root, file)\n",
    "        filename, file_extension = os.path.splitext(file)\n",
    "        \n",
    "        if file_extension == '.json':\n",
    "            try:\n",
    "                with open(filepath) as json_data:\n",
    "                    d = json.load(json_data)\n",
    "                    \n",
    "                    # Extract 'body_part'\n",
    "                    target = d['tags'][0]['name'] if d['tags'] else None\n",
    "                    \n",
    "                    # Extract 'height' and 'width'\n",
    "                    height = d['size']['height']\n",
    "                    width = d['size']['width']\n",
    "\n",
    "                    # Extract image filepath\n",
    "                    img_path = filepath.replace('ann', 'img')\n",
    "                    img_path = img_path.replace('.json', '')\n",
    "                    img_path = img_path.replace(\"./\", \"\")\n",
    "                    img_path = os.path.join(root_dir, img_path)\n",
    "\n",
    "                    # Extract file type - test, train, val\n",
    "                    ttv_type = os.path.dirname(filepath)\n",
    "                    ttv_type = os.path.dirname(ttv_type)\n",
    "                    ttv_type = ttv_type[2:]\n",
    "                    \n",
    "                    # Add the extracted data to the DataFrame\n",
    "                    fracdata = fracdata._append({'image_file_name': filename, 'File_path': img_path, 'ttv_type': ttv_type, 'Target': target, 'height': height, 'width': width, 'dataset': 'frac'}, ignore_index=True)\n",
    "            except Exception as e:\n",
    "                print(f'Cannot extract json values for {filename}: {e}')\n",
    "        else:\n",
    "            print(f'{filename} not a json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cf1f1b-5546-415e-8d8d-93bfb903d2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fracdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a45ff1-4d5b-4328-b788-15c712416a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Body parts:\")\n",
    "print(fracdata['Target'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fc3c67-e464-4914-ade5-bc79527033e1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### UNIFEST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c10437b-3392-4fcb-9b30-33fe2f42e52f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Collate images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987831cd-4350-4566-87c8-6835cd5df5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(root_dir)\n",
    "os.getcwd()\n",
    "\n",
    "os.makedirs(total_dir, exist_ok=True)\n",
    "\n",
    "uni_train = os.path.join(uni_dir, 'train')\n",
    "uni_test = os.path.join(uni_dir, 'test')\n",
    "frac_train = os.path.join(frac_dir, 'train')\n",
    "frac_test = os.path.join(frac_dir, 'test')\n",
    "frac_val = os.path.join(frac_dir, 'val')\n",
    "total_train = os.path.join(total_dir, 'train')\n",
    "total_test = os.path.join(total_dir, 'test')\n",
    "total_val = os.path.join(total_dir, 'val')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ada908-040d-4371-89f3-1ca4a86970c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# function to move images\n",
    "def move_images(source_root, destination_root):\n",
    "    for root, dirs, files in os.walk(source_root):\n",
    "        os.makedirs(destination_root, exist_ok=True)\n",
    "        \n",
    "        # Move each file to the destination folder\n",
    "        for file_name in files:\n",
    "            # Ignore hidden/.DStore files\n",
    "            if file.startswith(\".\"):\n",
    "                continue\n",
    "            \n",
    "            source_file_path = os.path.join(root, file_name)\n",
    "            destination_file_path = os.path.join(destination_root, file_name)\n",
    "            \n",
    "            # Move the file\n",
    "            try:\n",
    "                shutil.move(source_file_path, destination_file_path)\n",
    "            except Exception as e:\n",
    "                print(f'Could not move file {file_name}, encountered error {e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd4b617-1907-4083-bdc2-f52df193a354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNI dataset\n",
    "# move images from 'uni' archive folder to new total_images/train folder\n",
    "\n",
    "move_images(uni_train, total_train)\n",
    "\n",
    "os.chdir(total_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed03bef-85d5-4868-9514-c7aea006e703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of train dicoms\n",
    "!find . -type f -name \"*.dcm\" | wc -l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084c4afa-8c5a-46f5-b84d-aa0180114282",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "uni_val_no = int(round(0.125*1738, 0))\n",
    "print(f'Number of UNI training files for validation: {uni_val_no}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f945ddc-bf64-48d1-a5da-33051a16f94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take some of the UNI training set and use for validation:\n",
    "\n",
    "import random\n",
    "\n",
    "if os.path.exists(total_val):\n",
    "    print('UNIFEST validation set already created')\n",
    "else:\n",
    "    print('Creating UNIFEST validation set...')\n",
    "    os.makedirs(total_val, exist_ok=True)\n",
    "    dirContents = os.listdir(total_train)\n",
    "    selected_files = random.sample(dirContents, uni_val_no)\n",
    "    \n",
    "    for file_name in selected_files:\n",
    "        source_file_path = os.path.join(total_train, file_name)\n",
    "        destination_file_path = os.path.join(total_val, file_name)\n",
    "        shutil.move(source_file_path, destination_file_path)\n",
    "    \n",
    "    print(f\"Moved {len(selected_files)} files to {total_val}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6349b63a-bdcf-4dc3-8627-2213d511eca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the rest of the files over\n",
    "\n",
    "move_images(uni_test, total_test)\n",
    "move_images(frac_train, total_train)\n",
    "move_images(frac_test, total_test)\n",
    "move_images(frac_val, total_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7be43f1-c537-4adc-8d26-21d30b6d1f61",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Get labels for UNI train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a0e74d-55a0-4044-8926-5a1a8b6e9709",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.chdir(total_dir)\n",
    "os.getcwd()\n",
    "\n",
    "uni_data = pd.DataFrame(columns=['image_file_name', 'Target', 'height', 'width', 'File_path', 'ttv_type', 'dataset'])\n",
    "\n",
    "data_df = pd.read_csv(f'{uni_dir}/train.csv')\n",
    "data_df = data_df.rename(columns = {'SOPInstanceUID' : 'image_file_name'})\n",
    "data_df.head\n",
    "\n",
    "for root, dirs, filenames in os.walk(\".\"):\n",
    "\n",
    "    for file in filenames:      \n",
    "        if file.startswith(\".\"):\n",
    "            continue\n",
    "\n",
    "        filepath = os.path.join(root, file)\n",
    "        filename, file_extension = os.path.splitext(file)\n",
    "        filename = filename[:-2]\n",
    "        \n",
    "        if file_extension == '.dcm':\n",
    "            \n",
    "            # Extract 'body_part'\n",
    "            target_row = data_df[data_df['image_file_name'] == filename]\n",
    "            if not target_row.empty:\n",
    "                target = target_row['Target'].values[0]\n",
    "            else:\n",
    "                target = None\n",
    "                \n",
    "            # Get image dimensions\n",
    "            try:\n",
    "                dicom = pydicom.dcmread(filepath)\n",
    "                height = dicom.Rows\n",
    "                width = dicom.Columns\n",
    "            except:\n",
    "                print(f'Could not open dicom {filename}')\n",
    "                height = None\n",
    "                width = None\n",
    "\n",
    "            # Extract file type - test, train, val\n",
    "            if 'test' in str(filepath):\n",
    "                ttv_type = 'test'\n",
    "            elif 'train' in str(filepath):\n",
    "                ttv_type = 'train'\n",
    "            elif 'val' in str(filepath):\n",
    "                ttv_type = 'val'\n",
    "            else:\n",
    "                ttv_type = None\n",
    "\n",
    "            filepath = filepath.replace(\"./\", \"\")\n",
    "            filepath = os.path.join(root_dir, filepath)\n",
    "            \n",
    "            # Add the extracted data to the DataFrame\n",
    "            uni_data = uni_data._append({'image_file_name': filename, 'File_path': filepath, 'ttv_type': ttv_type, 'Target': target, 'height': height, 'width': width, 'dataset': 'uni'}, ignore_index=True)\n",
    "\n",
    "print(uni_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d5d2f1-da44-4ee4-9680-7bc9d90ace5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Unique values before remapping:')\n",
    "print(uni_data['Target'].unique())\n",
    "      \n",
    "train_data = uni_data[uni_data['ttv_type'] != 'test']\n",
    "train_data['Target'] = train_data['Target'].str.replace(' ', '').astype(int)\n",
    "train_data['Target'] = train_data['Target'].apply(lambda x: 22 if x > 21 else x)\n",
    "\n",
    "uni_data.update(train_data)\n",
    "\n",
    "print(f'\\nUnique values after remapping:')\n",
    "print(uni_data['Target'].unique())\n",
    "\n",
    "uni_data['Target'] = uni_data['Target'].fillna('None')\n",
    "uni_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fe82d4-f20f-4773-8337-2fa2212169ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "body_dict = {\n",
    "    0: \"abdomen\",\n",
    "    1: \"akle\",\n",
    "    2: \"cervical spine\",\n",
    "    3: \"chest\",\n",
    "    4: \"clavicles\",\n",
    "    5: \"elbow\",\n",
    "    6: \"feet\",\n",
    "    7: \"finger\",\n",
    "    8: \"forearm\",\n",
    "    9: \"hand\",\n",
    "    10: \"hip\",\n",
    "    11: \"knee\",\n",
    "    12: \"leg\",\n",
    "    13: \"lumbar spine\",\n",
    "    14: \"others\",\n",
    "    15: \"pelvis\",\n",
    "    16: \"shoulder\",\n",
    "    17: \"sinus\",\n",
    "    18: \"skull\",\n",
    "    19: \"thigh\",\n",
    "    20: \"thoracic spine\",\n",
    "    21: \"wrist\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b519ab9f-c301-4d6d-acf9-7e8e424a4448",
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_data['Target'] = uni_data['Target'].map(body_dict)\n",
    "print(uni_data['Target'].unique())\n",
    "uni_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580214a0-8229-4665-81b0-0fe3cedf09f0",
   "metadata": {},
   "source": [
    "#### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eee0e97-7f54-41fc-b5ad-216d3d528fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm image totals\n",
    "os.chdir(total_dir)\n",
    "!du sh \"test\"\n",
    "!du sh \"train\"\n",
    "!du sh \"val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb97814-5314-48a7-9f36-26c1f61de9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize all jpgs and resave\n",
    "# remove jsons\n",
    "from PIL import Image\n",
    "\n",
    "for root, dirs, files in os.walk(total_dir):\n",
    "    for file in files:\n",
    "        \n",
    "        file_path = os.path.join(root, file)\n",
    "        filename, file_extension = os.path.splitext(file)\n",
    "        if file_extension.lower() == '.jpg' or file_extension.lower() == '.jpeg':\n",
    "            try:\n",
    "                img = Image.open(file_path)\n",
    "                img_resized = img.resize((255, 255), Image.LANCZOS)\n",
    "                img_resized.save(file_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Error converting {file_path}: {e}\") \n",
    "        elif file_extension =='.json':\n",
    "            os.remove(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd255a5-0bc8-46f8-82a9-83611e80f109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DICOMs to jpgs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for root, dirs, files in os.walk(total_dir):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        filename, file_extension = os.path.splitext(file)\n",
    "        if file_extension.lower() == '.dcm':\n",
    "            try:\n",
    "                dcm = pydicom.dcmread(file_path)\n",
    "                pixel_array = dcm.pixel_array\n",
    "                new_filename = filename + \".jpg\"\n",
    "                new_filename = new_filename.replace('-c', \"\")\n",
    "\n",
    "                new_file_path = os.path.join(root, new_filename)\n",
    "                plt.imsave(new_file_path, pixel_array, cmap='gray')\n",
    "                \n",
    "                os.remove(file_path)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error converting {file_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad8a5e6-c748-491a-9a31-0cf802148202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-confirm image totals\n",
    "!du sh \"test\"\n",
    "!du sh \"train\"\n",
    "!du sh \"val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644a9638-99d8-47af-9a54-bce8cb51fcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def plot_mean_image_intensity_histogram(directory, bins=50):\n",
    "    mean_intensities = []\n",
    "\n",
    "    # Walk through each directory, subdirectory, and file\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            if filename.lower().endswith(('.jpg', '.jpeg')):\n",
    "                filepath = os.path.join(root, filename)\n",
    "                try:\n",
    "                    # Open the image\n",
    "                    img = Image.open(filepath).convert('L')  # Convert image to grayscale\n",
    "                    # Convert image to numpy array\n",
    "                    img_array = np.array(img)\n",
    "                    \n",
    "                    # Check if img_array is empty or has unexpected shape\n",
    "                    if img_array.size == 0:\n",
    "                        print(f\"Image array is empty: {filepath}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Calculate the mean intensity\n",
    "                    mean_intensity = img_array.mean()\n",
    "                    \n",
    "                    # Collect mean intensity value\n",
    "                    mean_intensities.append(mean_intensity)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {filepath}: {e}\")\n",
    "                    continue\n",
    "\n",
    "    # Debugging print\n",
    "    print(f\"Number of images processed: {len(mean_intensities)}\")\n",
    "    print(f\"Mean intensities range from {min(mean_intensities, default=0)} to {max(mean_intensities, default=0)}\")\n",
    "\n",
    "    # Plot the histogram of mean intensity values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(mean_intensities, bins=bins, color='blue', edgecolor='black')\n",
    "    plt.title('Histogram of Mean Image Intensity Values')\n",
    "    plt.xlabel('Mean Intensity Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "plot_mean_image_intensity_histogram(total_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6cedcf-c3b5-4ff3-aefa-a576e4a95883",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a13112e-e4e4-4666-be35-0437689b92aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_files = [f for f in os.listdir(total_train) if f.lower().endswith(('.jpg', '.jpeg'))]\n",
    "for filename in image_files:\n",
    "    img_path = os.path.join(total_train, filename)\n",
    "    img = Image.open(img_path)\n",
    "    img.convert('L')\n",
    "    \n",
    "    img.save(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98828f39-86ba-41b8-ab73-2f17bf9ad0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def showimg(original_img, augmented_img):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    \n",
    "    # Display Original Image\n",
    "    ax[0].imshow(original_img)\n",
    "    ax[0].set_title('Original')\n",
    "    ax[0].axis('off')\n",
    "    \n",
    "    # Display Augmented Image\n",
    "    ax[1].imshow(augmented_img)\n",
    "    ax[1].set_title('Augmented')\n",
    "    ax[1].axis('off')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e194a449-8203-4a72-8d74-36b4f8a9a4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import skimage.util\n",
    "\n",
    "def random_rotation(img):\n",
    "    \"\"\"\n",
    "    Rotate image by a random direction of 6 degrees\n",
    "    \"\"\"\n",
    "    direction = random.choice([1, -1])\n",
    "    return img.rotate(6 * direction)\n",
    "\n",
    "def random_noise(img):\n",
    "    \"\"\"\n",
    "    Add some noise to the image\n",
    "    \"\"\"\n",
    "    img_array = np.array(img)\n",
    "    noisy_img_array = skimage.util.random_noise(img_array, mode='gaussian', var=0.001)\n",
    "    return Image.fromarray((noisy_img_array * 255).astype(np.uint8))\n",
    "\n",
    "def horizontal_flip(img):\n",
    "    \"\"\"\n",
    "    Flip image horizontally\n",
    "    \"\"\"\n",
    "    return img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "\n",
    "def zoom(img, lim=12):\n",
    "    \"\"\"\n",
    "    Zoom image to account for rotation\n",
    "    \"\"\"\n",
    "    width, height = img.size\n",
    "    img = img.crop((lim, lim, width - lim, height - lim))\n",
    "    return img.resize((width, height), Image.LANCZOS)\n",
    "\n",
    "def apply_image_augmentation(img, scaling_size):\n",
    "    \"\"\"\n",
    "    Applying augmentation to image\n",
    "    \"\"\"\n",
    "    transformations = [horizontal_flip, random_rotation, zoom]\n",
    "    for transform in transformations:\n",
    "        img = transform(img)\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7e6d9f-1869-4762-8d6b-05b78127af03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of all image files in the directory\n",
    "image_files = [f for f in os.listdir(total_train) if f.lower().endswith(('.jpg', '.jpeg'))]\n",
    "total_images = len(image_files)\n",
    "\n",
    "# Calculate the number of images to replace\n",
    "num_images_to_replace = int(total_images * 0.1)\n",
    "\n",
    "# Randomly select images to replace\n",
    "images_to_replace = random.sample(image_files, num_images_to_replace)\n",
    "\n",
    "for filename in images_to_replace:\n",
    "    img_path = os.path.join(total_train, filename)\n",
    "    img = Image.open(img_path)\n",
    "    \n",
    "    # Apply augmentation\n",
    "    augmented_img = apply_image_augmentation(img, 1.5)\n",
    "    showimg(img, augmented_img)\n",
    "    \n",
    "    # Save augmented image (replacing the original one)\n",
    "    augmented_img.save(img_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5bfb9b-426f-44ba-8a13-0cd0a5bba80c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402c1ac9-aed0-4e49-be7a-de4f9ed391f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data = pd.concat([fracdata, uni_data], ignore_index=True)\n",
    "total_data = total_data.dropna()\n",
    "total_data = total_data[total_data['Target'] != 'nan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e4eff6-566d-4394-acf0-a24628b5bb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def plot_histogram(df, column_name, plot_type, bins=10):\n",
    "    # Convert column to string type to avoid TypeError\n",
    "    data = df[column_name].astype(str)\n",
    "    \n",
    "    # Plot the histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(data, bins=bins, edgecolor='black')\n",
    "\n",
    "    ax = plt.gca()\n",
    "    for tick in ax.get_xticklabels():\n",
    "        tick.set_rotation(45)\n",
    "        tick.set_horizontalalignment('right')\n",
    "        tick.set_y(tick.get_position()[1] - 0.05)\n",
    "    \n",
    "    # Set the title and labels\n",
    "    plt.title(f'Count of {plot_type} Types - Total')\n",
    "    plt.xlabel(plot_type)\n",
    "    #plt.xticks(rotation=45)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "plot_histogram(total_data, 'Target', 'Body Part', bins=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8420ad02-c0a9-4d78-a48e-d3a956a3e2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = {'uni': '#1f77b4', 'frac': '#ff7f0e'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd56617-6605-4a18-b1d5-589bf9064747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "def plot_split(df, body_part_column, split_by_column, palette):\n",
    "    # Convert relevant columns to string type to avoid TypeError\n",
    "    df[body_part_column] = df[body_part_column].astype(str)\n",
    "    df[split_by_column] = df[split_by_column].astype(str)\n",
    "    \n",
    "    # Plot the count plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.countplot(data=df, x=body_part_column, hue=split_by_column, palette=palette)\n",
    "    \n",
    "    # Rotate x-axis labels and adjust the offset\n",
    "    ax = plt.gca()\n",
    "    for tick in ax.get_xticklabels():\n",
    "        tick.set_rotation(45)\n",
    "        tick.set_horizontalalignment('right')\n",
    "        tick.set_y(tick.get_position()[1] - 0.05)\n",
    "    \n",
    "    # Set the title and labels\n",
    "    plt.title(f'Count of Body Part Types by Dataset')\n",
    "    plt.xlabel('Body Part')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "plot_split(total_data, 'Target', 'dataset', palette)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff936b5-5c50-4615-adc9-caf971edfe27",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03ccad6-6754-4508-bd74-499f81ecb65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = total_data[total_data['ttv_type'] == 'train']\n",
    "train_df = train_df.dropna(subset=['Target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197e3965-4934-4012-a285-333233700358",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df['Target'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19ba25a-29b3-4b6f-8bf7-5df132f4ed59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08acbf5-5f07-4963-bcf9-47296763b1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(train_df['Target'])\n",
    "\n",
    "x_train = []\n",
    "for file in train_df['image_file_name']:\n",
    "    if not file.endswith('.jpg'):\n",
    "        file = file + '.jpg'\n",
    "    img_path = os.path.join(total_train, file)\n",
    "    img = load_img(img_path, target_size=(255, 255))\n",
    "    img = img_to_array(img)\n",
    "    x_train.append(img)\n",
    "x_train = np.array(x_train)\n",
    "\n",
    "val_df = total_data[total_data['ttv_type'] == 'val']\n",
    "val_df = val_df.dropna(subset=['Target'])\n",
    "\n",
    "x_val = []\n",
    "for file in val_df['image_file_name']:\n",
    "    if not file.endswith('.jpg'):\n",
    "        file = file + '.jpg'\n",
    "    img_path = os.path.join(total_val, file)\n",
    "    img = load_img(img_path, target_size=(255, 255))\n",
    "    img = img_to_array(img)\n",
    "    x_val.append(img)\n",
    "x_val = np.array(x_val)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cecb491-9406-4a12-af1c-696d22771f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(255, 255, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),  \n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(64, activation='relu'),  \n",
    "    Dense(32, activation='relu'),  \n",
    "    Dense(len(le.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "optimizer = SGD(learning_rate=1e-4, momentum=0.9)\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87677a99-c32a-493c-83fc-c6e12d8b7bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, epochs=10, validation_data=(x_val, y_val))\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0839a04d-7876-4b0d-ac75-e6484ba2b1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = total_data[total_data['ttv_type'] == 'test']\n",
    "test_df = test_df.dropna(subset=['Target'])\n",
    "\n",
    "test_images = []\n",
    "test_labels = []\n",
    "test_filenames = []\n",
    "\n",
    "for _, row in test_df.iterrows():\n",
    "    img_path = os.path.join(total_test, row['image_file_name'])\n",
    "    img = load_img(img_path, target_size=(255, 255))\n",
    "    img = img_to_array(img)\n",
    "    test_images.append(img)\n",
    "    test_labels.append(row['Target'])\n",
    "    test_filenames.append(row['image_file_name'])\n",
    "\n",
    "test_labels = le.transform(test_labels)\n",
    "test_images = np.array(test_images)\n",
    "test_loss, test_accuracy = model.evaluate(test_images, test_labels)\n",
    "print(f'Test accuracy: {test_accuracy}\\n')\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(test_images)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "predicted_labels = le.inverse_transform(predicted_labels)\n",
    "pred_df = pd.DataFrame({'filename': test_filenames, 'predicted_labels': predicted_labels})\n",
    "\n",
    "empty_count = pred_df['predicted_labels'].isna().sum()\n",
    "print(f'Number of empty predictions: {empty_count}')\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "target_names = le.classes_\n",
    "report = classification_report(test_labels, predicted_labels, target_names=target_names)\n",
    "print(report)\n",
    "\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(test_labels, predicted_labels, average='weighted')\n",
    "print(f'Precision: {precision}\\nRecall: {recall}\\nF1 Score: {f1}\\n')\n",
    "\n",
    "# Save the model\n",
    "model.save(\"cnn.keras\")\n",
    "model.save(\"cnn.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7810314-e016-4654-aeb8-57dfc081b9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pydot\n",
    "#!pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00894b4a-10b5-4928-b16f-5f16233ef3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "\n",
    "def show_dcm_predict(index, pred_df, test_df):\n",
    "    filename = pred_df['filename'].iloc[index]\n",
    "    \n",
    "    img = load_img(filename)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f'Filename: {filename}')\n",
    "    row = pred_df.iloc[index]\n",
    "    \n",
    "    if row is not None:\n",
    "        predicted_label = row['predicted_labels']\n",
    "    else:\n",
    "        print('No predicted label found')\n",
    "\n",
    "    actual_label_row = test_df[test_df['image_file_name'] == os.path.basename(filename)]\n",
    "    \n",
    "    if not actual_label_row.empty:\n",
    "        actual_label = actual_label_row['Target'].values[0]\n",
    "    else:\n",
    "        actual_label = 'No actual label found'\n",
    "    \n",
    "    print(f'Predicted label: {predicted_label}')\n",
    "    print(f'Actual label: {actual_label}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6bd945-5523-45b9-8acf-9af26003a9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(total_test)\n",
    "    \n",
    "predict_label_1 = show_dcm_predict(44, pred_df, test_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
